{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gensim\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN\n",
    "#import classla\n",
    "#import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some utility functions which are used\n",
    "def cos_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "def remove_diplicate_sentences(a):\n",
    "    b_set = set(map(tuple,a))  #need to convert the inner lists to tuples so they are hashable\n",
    "    b = list(map(list,b_set)) #Now convert tuples back into lists (maybe unnecessary?)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading word2vec skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = \"../../\"\n",
    "with zipfile.ZipFile(repository + \"/67.zip\", \"r\") as archive:\n",
    "    stream = archive.open(\"model.txt\")\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=False, unicode_errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picokih: 0.6543\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "result = model.similar_by_word(\"legenda\")\n",
    "most_similar_key, similarity = result[0]  \n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['leto', 'dan', 'konec', 'svet', 'stran', 'mesto', 'šola', 'ura',\n",
       "       'beseda', 'pot', 'red', 'zakon', 'zadeva', 'srce', 'tema',\n",
       "       'resnica', 'moški', 'vloga', 'kraj', 'stanje', 'škoda', 'film',\n",
       "       'večer', 'vrh', 'jutro', 'kazen', 'oblast', 'račun', 'novica',\n",
       "       'milijon', 'par', 'krog', 'tip', 'punca', 'sila', 'vir', 'las',\n",
       "       'akcija', 'meter', 'prst', 'kri', 'stik', 'grad', 'znak', 'lik',\n",
       "       'direktor', 'vodja', 'raven', 'kolo', 'rob', 'gost', 'duh',\n",
       "       'praznik', 'vest', 'korist', 'vedenje', 'tek', 'kup', 'otok',\n",
       "       'razstava', 'bitje', 'motor', 'karta', 'nevarnost', 'hitrost',\n",
       "       'kos', 'zob', 'stroj', 'kamen', 'župan', 'šef', 'vrtec', 'kot',\n",
       "       'deček', 'avgust', 'tok', 'jezero', 'klop', 'čelo', 'hip', 'kupec',\n",
       "       'pojav', 'čaj', 'postava', 'dolg', 'standard', 'jesen', 'rak',\n",
       "       'grob', 'plus', 'les', 'vez', 'polica', 'minus', 'plan', 'posoda',\n",
       "       'restavracija', 'jok', 'krilo', 'sol', 'rod', 'stres', 'trditev',\n",
       "       'faks', 'sled', 'gol', 'župnik', 'servis', 'prid', 'ustava',\n",
       "       'mora', 'pop', 'lokal', 'prevod', 'bazen', 'veda', 'plaža',\n",
       "       'pesek', 'opravljanje', 'bolnica', 'hoja', 'raj', 'maček', 'baza',\n",
       "       'kocka', 'sejem', 'bar', 'hodnik', 'lisica', 'skala', 'lev', 'vek',\n",
       "       'talent', 'peč', 'termin', 'krivec', 'lega', 'tableta',\n",
       "       'pogajanje', 'obrat', 'kampanja', 'očka', 'med', 'stop', 'plast',\n",
       "       'golf', 'mina', 'kip', 'pod', 'para', 'prilika', 'slovo', 'piknik',\n",
       "       'boja', 'krajina', 'nona', 'odpuščanje', 'draga', 'faktor', 'bit',\n",
       "       'vodnik', 'razgled', 'popoldan', 'pol', 'obresti', 'paša', 'pust',\n",
       "       'teza', 'koda', 'predsodek', 'instrument', 'smuč', 'pečica',\n",
       "       'nestrpnost', 'lok', 'bob', 'obrt', 'kopanje', 'rez', 'pok',\n",
       "       'jarek', 'peščica', 'metal', 'palček', 'ruta', 'solo', 'hod'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../ccGigaFida/results/data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "with open('../ccGigaFida/results/data_lema.json') as json_file:\n",
    "    data_lema = json.load(json_file)\n",
    "with open('../ccGigaFida/results/data_pos.json') as json_file:\n",
    "    data_len = json.load(json_file)\n",
    "\n",
    "words = np.load(\"../ccGigaFida/words.npy\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'drag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-d43ec0cb1fd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'drag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mall_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mall_sentences_lema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_lema\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#Remove duplicate sentences if any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'drag'"
     ]
    }
   ],
   "source": [
    "for keyword in ['drag']:\n",
    "    all_sentences=data[keyword][0:1000]\n",
    "    all_sentences_lema=data_lema[keyword][0:1000]\n",
    "    \n",
    "    #Remove duplicate sentences if any\n",
    "    #all_sentences=remove_diplicate_sentences(all_sentences)\n",
    "    #all_sentences_lema=remove_diplicate_sentences(all_sentences_lema)\n",
    "    \n",
    "    all_embeddings=np.zeros((len(all_sentences),100))\n",
    "    \n",
    "    for i in range(len(all_sentences)): #iterate through the sentences for the given keyword\n",
    "        keyword_position = all_sentences_lema[i].index(keyword)\n",
    "        #all_sentences[i].pop(keyword_position)\n",
    "        \n",
    "        centroid=np.zeros(100)\n",
    "        words_added=0\n",
    "        \n",
    "        for word_index, word in enumerate(all_sentences[i]): #iterate through the words in the sentence\n",
    "            try:\n",
    "                if word_index!=keyword_position:\n",
    "                    centroid+=model[word.lower()] \n",
    "                    words_added+=1\n",
    "            except:\n",
    "                ...\n",
    "                \n",
    "        if words_added==0: #there are wierd sentences where only the keyword is the whole sentence\n",
    "            continue\n",
    "            \n",
    "        centroid/=words_added\n",
    "        all_embeddings[i,:]=centroid\n",
    "        \n",
    "    \n",
    "    break #this break means that we terminate on the first word\n",
    "    \n",
    "#~np.all(all_embeddings == 0, axis=1) this checks if there are any only-zero rows meaning that we did not have any word embedding for that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-cca5a08a4d16>:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n"
     ]
    }
   ],
   "source": [
    "distances={}\n",
    "for i in range(all_embeddings.shape[0]):\n",
    "    for j in range(i+1,all_embeddings.shape[0],1):\n",
    "        distances[str(i)+'-'+str(j)] = cos_similarity(all_embeddings[i,:], all_embeddings[j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = dict(sorted(distances.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze worst 50 distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Med najbolj znanimi misijonarji slovenskega rodu je bil vsekakor Ignacij Knoblehar misijonar v Sudanu\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "Posamezni deli so namreč danes zelo nepovezani med sabo\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "Med gradbenimi podvigi naj omenimo čudovit baročni dvorec v Goričanah ki sta ga zidala in opremila škofa Oton Friderik grof Buchheim in Jožef grof Rabatta novo ljubljansko stolnico ki jo je leta 1707 posvetil škof Ferdinand Franc grof Kuenburg in veličastno cerkev sv. Mohorja in Fortunata v Gornjem Gradu ki jo je sredi 18. stoletja dal postaviti škof Ernest Amadej grof Attems\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "Lačnova že med 50\n",
      "MED Sporazum\n",
      "-----\n",
      "Med najpomembnejše pionirje atomskih bomb spadata zlasti ameriški fizik J. Robert Oppenheimer znanstveni vodja projekta Manhattan\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "Ko so sodili nacističnim zločincem v Nürnbergu so bili med sodniki tudi sovjetski ki bi morali najprej pomesti pred svojim pragom\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "Sodelovanje med materjo in sinom je bilo oteženo delno zaradi generacijske razlike delno zaradi drugačnih idej\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "Te osebnosti so v skoraj 550 letni zgodovini ljubljanske škofije odločilno posegle v versko in cerkveno družbeno in kulturno politično in cerkvenopolitično življenje domala celotnega slovenskega prostora nekateri med njimi pa so odigrali pomembno vlogo tudi na političnem in diplomatskem parketu Svetega rimskega cesarstva nemške narodnosti\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "Predvsem so v oči bodla diametralno nasprotna stališča med Jelinčičem in Pečetom pri posameznih vprašanjih\n",
      "TOČENJE MEDU\n",
      "-----\n",
      "A odnos med državama ni bistvenega pomena le na gospodarskem področju le skupaj lahko Gruzija in Rusija rešita problematiko dveh separatističnih pokrajin v Gruziji Južne Osetije in Abhazije\n",
      "TOČENJE MEDU\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "distances_keys = list(distances.keys())\n",
    "already_were = [] #to je zato ker se stavke ponavljajo in ne zanimajo nas iste kombinacije znova\n",
    "count=0\n",
    "for key in distances_keys[0:100]:\n",
    "    first, second = map(int, key.split('-'))\n",
    "    first_sent, second_sent = ' '.join(all_sentences[first]), ' '.join(all_sentences[second])\n",
    "    \n",
    "    if first_sent+second_sent in already_were:\n",
    "        continue\n",
    "    \n",
    "    count+=1\n",
    "    already_were.append(first_sent+second_sent)\n",
    "    print(first_sent+'\\n'+second_sent)\n",
    "    print('-----')\n",
    "    if count==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze best 50 distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vroča diskusija se je končala s sklepom da bo Odbor pozdravil ustanovitev SDZ ne bo pa med soustanovitelji\n",
      "Tiskovni predstavnik Evropske komisije mu je takrat odgovoril da je kandidatk za članstvo že veliko a med njimi ni Izraela in Rusije\n",
      "-----\n",
      "Ko je bil v generalni hiši v Rimu med leti 1944 in 1946 je bil tam iz Slovenije neki komunist za katerega pater Tomaž ni vedel kakšnih nazorov je\n",
      "Mešano množico v kateri je bilo tudi veliko takih ki so čisto naključno med sprehodom po Izoli ugotovili da se na Manziolijevem trgu nekaj dogaja je navdušil tudi Tartinijev Koncert za trobento v katerem je kot solist nastopil Stanko Arnold\n",
      "-----\n",
      "V tem času pa je pri izoblikovanju meril sodelovalo veliko strokovnjakov med njimi prav precej takih ki so vse svoje življenje posvetili psom\n",
      "Na ta način ne bo časa da bi kakšnega med slovenskimi škofi pohvalil ali posebno pokaral\n",
      "-----\n",
      "Med Črnovrščani je bilo pred časom slišati da naj bi ga kupilo šolsko ministrstvo za seminarje šolnikov in za obšolske dejavnosti najnovejše informacije pa pravijo da se zanj zanima zdomka iz Nemčije\n",
      "Če bi bile volitve jutri ne bi šlo za odločanje med boljšim in slabšim ampak za odločanje med znanim in starim\n",
      "-----\n",
      "Kučan je v nadaljevanju debate namignil na Lazarja Mojsova in njegovo obtožbo da za razglabljanji o republiški armadi stoji Ivan Maček Matija in si med pisanjem zapisnika pribeležil ministrov odgovor Zatrdil je da ta zadeva ni v nikakršni zvezi z njegovim sklepanjem o ustvarjanju predpostavk o formiranju republiške armade v Sloveniji\n",
      "Med ednodnevnim uradnim obiskom je vodja slovaške diplomacije podpisal sporazum o sodelovanju med državama v kombiniranem prevozu in program o sodelovanju v izobraževanju znanosti in kulturi za obdobje od 1999 do 2000\n",
      "-----\n",
      "Že zdaj pa je jasno da brez dogovora med tekmecema za Union Laščani in Belgijci ne bo šlo\n",
      "Organizatorji so si dovolili veliko nerodnost ko so navijače SČG med njimi sta bila tudi Radko Varda in Radoslav Nesterović spustili na novinarsko tribuno saj so tja po koncu tekme namreč vdrli Hrvati in eden izmed navijačev je celo padel tri metre globoko\n",
      "-----\n",
      "Pakistanskemu predsedniku generalu Pervezu Mušarafu desno ki je po prvem pogovoru z ameriškimi diplomati izjavil da nič kar so mu povedali ne dokazuje zveze med Osamom bin Ladnom in napadi na ZDA je britanski premier Tony Blair levo dal v roke šop dokumentov\n",
      "Godard Rivette in večno sijajni Rohmer so letos pometli z mlajšimi tekmeci lani pa je nekaj podobnega uspelo celo Claudu Chabrolu najšibkejšemu oziroma najmanj konsistentnemu členu med preživelimi teoretiki cineasti\n",
      "-----\n",
      "Še največ so govorili o neuspešni primopredaji poslov in dokumentacije med Giom in Smeltom Intagom v Švici ki bi jo morali opraviti po odstopu uprave ki jo je vodil nekdanji dolgoletni direktor Smelta in sedanji večinski lastnik zasebne družbe Smelt International Jože Žagar\n",
      "V Washingtonu je podpisal več sporazumov o denarni pomoči med drugim tudi sporazum s Svetovno banko o razvoju kmetijstva in majhnih podjetij\n",
      "-----\n",
      "Diahronična lingvistika pa mora narobe proučevati razmerja med elementi ki si sledijo drug za drugim in ki ne obstajajo hkrati za isto družbeno zavest ti elementi zamenjujejo drug drugega v času med seboj pa ne oblikujejo nobenega sistema 29\n",
      "Reditelj ne more preprečiti in urejati sporov med učenci včasih jih s svojim napačnim ravnanjem lahko celo zaplete in stopnjuje\n",
      "-----\n",
      "Tuji govor v starofrancoskem jeziku V starofrancoskem jeziku se psihološke in gramatikalne konstrukcije med seboj še zdaleč niso razlikovale tako močno kakor danes\n",
      "S tem v zvezi je opozoril na izjave ki jih je bilo slišati v Sloveniji da Hrvaška nikakor ne more biti med tistimi državami ki bi oblikovale takšno skupnost\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "distances_keys = list(distances.keys())\n",
    "already_were = [] #to je zato ker se stavke ponavljajo in ne zanimajo nas iste kombinacije znova\n",
    "count=0\n",
    "for key in distances_keys[-1000:]:\n",
    "    first, second = map(int, key.split('-'))\n",
    "    first_sent, second_sent = ' '.join(all_sentences[first]), ' '.join(all_sentences[second])\n",
    "    if first_sent+second_sent in already_were:\n",
    "        continue\n",
    "    \n",
    "    count+=1\n",
    "    already_were.append(first_sent+second_sent)\n",
    "    print(first_sent+'\\n'+second_sent)\n",
    "    print('-----')\n",
    "    if count==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct similarity distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.13731799, 0.22660908, ..., 0.32688616, 0.24738726,\n",
       "        0.27633132],\n",
       "       [0.13731799, 0.        , 0.12635066, ..., 0.28019894, 0.18324466,\n",
       "        0.17706561],\n",
       "       [0.22660908, 0.12635066, 0.        , ..., 0.23151193, 0.20322849,\n",
       "        0.1742427 ],\n",
       "       ...,\n",
       "       [0.32688616, 0.28019894, 0.23151193, ..., 0.        , 0.32300266,\n",
       "        0.32060998],\n",
       "       [0.24738726, 0.18324466, 0.20322849, ..., 0.32300266, 0.        ,\n",
       "        0.29758845],\n",
       "       [0.27633132, 0.17706561, 0.1742427 , ..., 0.32060998, 0.29758845,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(all_embeddings.shape)\n",
    "distance_matrix=(pairwise_distances(all_embeddings,metric=\"cosine\"))\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = AgglomerativeClustering(affinity='precomputed', linkage='complete', n_clusters=2).fit(distance_matrix)\n",
    "clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marko\\anaconda3\\envs\\ids-clone\\lib\\site-packages\\sklearn\\cluster\\_spectral.py:484: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SpectralClustering(2).fit_predict(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated no. of clusters: 2\n",
      "Estimated no. of noise points: 0\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n",
      "['Priznanje', 'Podjetnik', 'leta']\n"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN(metric='cosine', eps=0.4, min_samples=3).fit(all_embeddings)  # you can change these parameters, given just for example \n",
    "labels = dbscan.labels_ # where X - is your matrix, where each row corresponds to one document (line) from the docs, you need to cluster \n",
    "#cluster_labels\n",
    "no_clusters = len(np.unique(labels) )\n",
    "no_noise = np.sum(np.array(labels) == -1, axis=0)\n",
    "\n",
    "print('Estimated no. of clusters: %d' % no_clusters)\n",
    "print('Estimated no. of noise points: %d' % no_noise)\n",
    "\n",
    "labels\n",
    "#all_sentences[np.where(labels == 1)]\n",
    "for i,label in enumerate(labels):\n",
    "    if label==1:\n",
    "        print(all_sentences[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids-clone",
   "language": "python",
   "name": "ids-clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
