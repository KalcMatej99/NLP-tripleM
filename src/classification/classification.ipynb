{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(results, name):\n",
    "    plus_minus = np.std(results) / np.sqrt(len(results)) * 1.96\n",
    "    mean = np.mean(results)\n",
    "    print(f'Mean of {name}: {round(mean,2)} and 95% CI: [{round(mean-plus_minus,2)}, {round(mean+plus_minus,2)}]')\n",
    "    f.write(f'Mean of {name}: {round(mean,2)} and 95% CI: [{round(mean-plus_minus,2)}, {round(mean+plus_minus,2)}]\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(25,9), sharey=True)\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.set_cmap(\"plasma\")\n",
    "\n",
    "count = 0\n",
    "words = ['faks', 'gol', 'golf', 'klop', 'rak']\n",
    "\n",
    "for l, file in enumerate(os.listdir('./data')):\n",
    "\n",
    "    if l%4 == 0:\n",
    "        log_loss_mean = []\n",
    "        log_loss_CI = []\n",
    "        baseline_mean = []\n",
    "        baseline_CI = []\n",
    "        tree_loss_mean = []\n",
    "        tree_CI = []\n",
    "\n",
    "    file_name = file[:-4]\n",
    "    f = open(f'results/{file_name}.txt', \"a\")\n",
    "\n",
    "    df = pd.read_csv(f'data/{file_name}.csv')\n",
    "    df_annotated = pd.read_csv(f'./annotated/{file_name}.csv')\n",
    "\n",
    "    k = 5\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X = df.iloc[:,:-1].values\n",
    "    y = df.iloc[:,-1].values\n",
    "\n",
    "    X_annotated = df_annotated.iloc[:,1:-1].values\n",
    "    y_annotated = df_annotated.iloc[:,-1].values\n",
    "\n",
    "\n",
    "    loss = []\n",
    "    baseline_loss = []\n",
    "    random_baseline_loss = []\n",
    "    ca = []\n",
    "    baseline_ca = []\n",
    "    random_ca = []\n",
    "    annotated_loss = []\n",
    "    annotated_ca = []\n",
    "\n",
    "    tree_loss = []\n",
    "\n",
    "    print(\"number of sentences: \", len(X), \"for: \", file_name)\n",
    "    for j in range(10):\n",
    "\n",
    "        kfold = KFold(k, shuffle=True, random_state=j)\n",
    "        splits = kfold.split(X)\n",
    "        \n",
    "        \n",
    "        for i in range(k):\n",
    "            # print(i)\n",
    "            split = next(splits, None)\n",
    "            X_cross_train = X[split[0]]\n",
    "            X_cross_test = X[split[1]]\n",
    "            y_cross_train = y[split[0]]\n",
    "            y_cross_test = y[split[1]]\n",
    "            X_cross_train = scaler.fit_transform( X_cross_train )\n",
    "            X_cross_test = scaler.transform( X_cross_test )\n",
    "\n",
    "            clf = LogisticRegression(random_state=i).fit(X_cross_train, y_cross_train)\n",
    "            predictions = clf.predict_proba(X_cross_test)\n",
    "            pred = clf.predict(X_cross_test)\n",
    "\n",
    "            rf = RandomForestClassifier(n_estimators = 10, random_state = i)\n",
    "            rf.fit(X_cross_train, y_cross_train)\n",
    "            predictions2 = rf.predict_proba(X_cross_test)\n",
    "            pred2 = rf.predict(X_cross_test)\n",
    "\n",
    "            zeros = len(y_cross_test[y_cross_test==0])\n",
    "            ones = len(y_cross_test[y_cross_test==1])\n",
    "            both = zeros + ones\n",
    "\n",
    "            if zeros == 0 or ones == 0:\n",
    "                print(\"error\")\n",
    "                continue\n",
    "\n",
    "            baseline_loss.append(log_loss(y_cross_test, np.hstack((np.array([zeros/both]*both).reshape(-1, 1),np.array([ones/both]*both).reshape(-1, 1)))))\n",
    "            random_baseline_loss.append(log_loss(y_cross_test, np.random.random([len(y_cross_test),2])))\n",
    "            loss.append(log_loss(y_cross_test, predictions))\n",
    "\n",
    "            baseline_ca.append(len(y_cross_test[y_cross_test==stats.mode(y_cross_test)[0][0]])/len(y_cross_test))\n",
    "            random_ca.append(len(y_cross_test[y_cross_test==np.random.randint(0,2,len(y_cross_test))])/len(y_cross_test))\n",
    "            ca.append(np.sum(pred==y_cross_test)/len(y_cross_test))\n",
    "\n",
    "            tree_loss.append(log_loss(y_cross_test, predictions2))\n",
    "\n",
    "            predictions = clf.predict_proba(X_annotated)\n",
    "            pred = clf.predict(X_annotated)\n",
    "\n",
    "            predictions = rf.predict_proba(X_annotated)\n",
    "            pred = rf.predict(X_annotated)\n",
    "\n",
    "            annotated_loss.append(log_loss(y_annotated, pred))\n",
    "            annotated_ca.append(np.sum(pred==y_annotated)/len(y_annotated))\n",
    "\n",
    "    print_info(random_baseline_loss, 'random_baseline_loss')\n",
    "    print_info(baseline_loss, 'baseline_loss')\n",
    "    print_info(loss, 'loss')\n",
    "    print_info(random_ca, 'random_CA')\n",
    "    print_info(baseline_ca, 'baseline_CA')\n",
    "    print_info(ca, 'CA')\n",
    "\n",
    "    print_info([np.mean(len(y_annotated[y_annotated==stats.mode(y_annotated)[0][0]])/len(y_annotated))], 'annotated baseline')\n",
    "    print_info(annotated_loss, \"annotated_loss\")\n",
    "    print_info(annotated_ca,\"annotated_ca\")\n",
    "\n",
    "    log_loss_mean.append(np.mean(loss))\n",
    "    log_loss_CI.append(np.std(loss) / np.sqrt(len(loss)) * 1.96)\n",
    "    baseline_mean.append(np.mean(baseline_loss))\n",
    "    baseline_CI.append(np.std(baseline_loss) / np.sqrt(len(baseline_loss)) * 1.96)\n",
    "    tree_loss_mean.append(np.mean(tree_loss))\n",
    "    tree_CI.append(np.std(tree_loss) / np.sqrt(len(tree_loss)) * 1.96)\n",
    "\n",
    "    if l%4 == 3:\n",
    "        # axs[count].errorbar(range(4), log_loss_mean, yerr=log_loss_CI, label='Log reg')\n",
    "        # axs[count].errorbar(range(4), tree_loss_mean, yerr=tree_CI, label='random forest')\n",
    "        # axs[count].errorbar(range(4), baseline_mean, yerr=baseline_CI, label='baseline')\n",
    "        barWidth = 0.25\n",
    "        br1 = range(4)\n",
    "        br2 = [x + barWidth for x in br1]\n",
    "        br3 = [x + barWidth for x in br2]\n",
    "        axs[count].bar(br1, log_loss_mean, width = barWidth,\n",
    "                edgecolor ='grey', label ='Logistic regression', yerr=log_loss_CI)\n",
    "        axs[count].bar(br2, tree_loss_mean, width = barWidth,\n",
    "                edgecolor ='grey', label ='Random forest', yerr=tree_CI)\n",
    "        axs[count].bar(br3, baseline_mean, width = barWidth,\n",
    "                edgecolor ='grey', label ='Baseline', yerr=baseline_CI)\n",
    "        axs[count].set_xticks([r + barWidth for r in range(4)])\n",
    "        axs[count].set_xticklabels(['ELMo', 'fastText', 'sloBERTa', 'Word2Vec'])\n",
    "        axs[count].set_ylim(0,0.7)\n",
    "        axs[count].set_title(words[count], fontsize=15, bbox={'facecolor':'0.9', 'pad':2})\n",
    "        \n",
    "        # plt.xl\n",
    "        count += 1\n",
    "\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.close()\n",
    "axs[2].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=3)\n",
    "axs[0].set_ylabel(\"Log-loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "004cf412bede8e677bcf59b14e1e48ce17b57f17e644433ba0ac6b9fba05c9bd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp-trimpleM': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
