{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoCharacterMapper\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from allennlp.modules.token_embedders.elmo_token_embedder import ElmoTokenEmbedder\n",
    "\n",
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from allennlp.data import Token, Vocabulary, TokenIndexer, Tokenizer\n",
    "from allennlp.data.fields import ListField, TextField\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedTransformerIndexer,\n",
    "    PretrainedTransformerMismatchedIndexer,\n",
    ")\n",
    "from allennlp.data.tokenizers import (\n",
    "    CharacterTokenizer,\n",
    "    PretrainedTransformerTokenizer,\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ElmoTokenEmbedder(\n",
    "    options_file=\"../slovenian-elmo/options.json\",\n",
    "    weight_file=\"../slovenian-elmo/slovenian-elmo-weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo tokens: [Kako, je, stari]\n",
      "ELMo tensors: {'elmo_tokens': {'elmo_tokens': tensor([[259,  76,  98, 108, 112, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 107, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 116, 117,  98, 115, 106, 260, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261]])}}\n",
      "ELMo embedded tokens: tensor([[[-0.1121, -0.0944, -0.0000,  ...,  0.1034, -0.0813,  0.2513],\n",
      "         [-0.0000,  0.0000,  0.0000,  ..., -0.3086,  0.0000, -0.1073],\n",
      "         [ 0.0000,  0.3551, -0.0000,  ..., -0.4340,  0.0000, -0.0000]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()\n",
    "vocab = Vocabulary()\n",
    "text = \"Kako je stari\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"ELMo tokens:\", tokens)\n",
    "text_field = TextField(tokens, {\"elmo_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "token_tensor = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"ELMo tensors:\", token_tensor)\n",
    "\n",
    "# We're using a tiny, toy version of ELMo to demonstrate this.\n",
    "elmo_embedding = ElmoTokenEmbedder(\n",
    "    options_file=\"../slovenian-elmo/options.json\",\n",
    "    weight_file=\"../slovenian-elmo/slovenian-elmo-weights.hdf5\")\n",
    "\n",
    "embedder = BasicTextFieldEmbedder(token_embedders={\"elmo_tokens\": elmo_embedding})\n",
    "\n",
    "tensor_dict = text_field.batch_tensors([token_tensor])\n",
    "embedded_tokens = embedder(tensor_dict)\n",
    "print(\"ELMo embedded tokens:\", embedded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Kako, je, stari]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[-0.0560, -0.0472, -0.0829,  ...,  0.0517, -0.0407,  0.1257],\n",
      "         [-0.2654,  0.2074,  0.1101,  ..., -0.1543,  0.2586, -0.0537],\n",
      "         [ 0.0599,  0.1776, -0.4385,  ..., -0.2170,  0.2408, -0.1374]],\n",
      "\n",
      "        [[-0.0560, -0.0472, -0.0829,  ...,  0.0517, -0.0407,  0.1257],\n",
      "         [-0.2654,  0.2074,  0.1101,  ..., -0.1543,  0.2586, -0.0537],\n",
      "         [ 0.0599,  0.1776, -0.4385,  ..., -0.2170,  0.2408, -0.1374]]],\n",
      "       grad_fn=<CopySlices>)]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file=\"../slovenian-elmo/options.json\",\n",
    "weight_file=\"../slovenian-elmo/slovenian-elmo-weights.hdf5\"\n",
    "# Note the \"1\", since we want only 1 output representation for each token.\n",
    "elmo = Elmo(options_file=\"../slovenian-elmo/options.json\",\n",
    "            weight_file=\"../slovenian-elmo/slovenian-elmo-weights.hdf5\", num_output_representations=1, dropout=0)\n",
    "\n",
    "# use batch_to_ids to convert sentences to character ids\n",
    "sentences = [['Kako', 'je', 'stari'], ['Kako', 'je', 'stari']]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids)\n",
    "print(embeddings['elmo_representations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-8e2a1142cf59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'elmo_representations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "embeddings['elmo_representations'][0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some utility functions which are used\n",
    "def cos_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "def remove_diplicate_sentences(a):\n",
    "    b_set = set(map(tuple,a))  #need to convert the inner lists to tuples so they are hashable\n",
    "    b = list(map(list,b_set)) #Now convert tuples back into lists (maybe unnecessary?)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['leto', 'dan', 'konec', 'svet', 'stran', 'mesto', 'šola', 'ura',\n",
       "       'beseda', 'pot', 'red', 'zakon', 'zadeva', 'srce', 'tema',\n",
       "       'resnica', 'moški', 'vloga', 'kraj', 'stanje', 'škoda', 'film',\n",
       "       'večer', 'vrh', 'jutro', 'kazen', 'oblast', 'račun', 'novica',\n",
       "       'milijon', 'par', 'krog', 'tip', 'punca', 'sila', 'vir', 'las',\n",
       "       'akcija', 'meter', 'prst', 'kri', 'stik', 'grad', 'znak', 'lik',\n",
       "       'direktor', 'vodja', 'raven', 'kolo', 'rob', 'gost', 'duh',\n",
       "       'praznik', 'vest', 'korist', 'vedenje', 'tek', 'kup', 'otok',\n",
       "       'razstava', 'bitje', 'motor', 'karta', 'nevarnost', 'hitrost',\n",
       "       'kos', 'zob', 'stroj', 'kamen', 'župan', 'šef', 'vrtec', 'kot',\n",
       "       'deček', 'avgust', 'tok', 'jezero', 'klop', 'čelo', 'hip', 'kupec',\n",
       "       'pojav', 'čaj', 'postava', 'dolg', 'standard', 'jesen', 'rak',\n",
       "       'grob', 'plus', 'les', 'vez', 'polica', 'minus', 'plan', 'posoda',\n",
       "       'restavracija', 'jok', 'krilo', 'sol', 'rod', 'stres', 'trditev',\n",
       "       'faks', 'sled', 'gol', 'župnik', 'servis', 'prid', 'ustava',\n",
       "       'mora', 'pop', 'lokal', 'prevod', 'bazen', 'veda', 'plaža',\n",
       "       'pesek', 'opravljanje', 'bolnica', 'hoja', 'raj', 'maček', 'baza',\n",
       "       'kocka', 'sejem', 'bar', 'hodnik', 'lisica', 'skala', 'lev', 'vek',\n",
       "       'talent', 'peč', 'termin', 'krivec', 'lega', 'tableta',\n",
       "       'pogajanje', 'obrat', 'kampanja', 'očka', 'med', 'stop', 'plast',\n",
       "       'golf', 'mina', 'kip', 'pod', 'para', 'prilika', 'slovo', 'piknik',\n",
       "       'boja', 'krajina', 'nona', 'odpuščanje', 'draga', 'faktor', 'bit',\n",
       "       'vodnik', 'razgled', 'popoldan', 'pol', 'obresti', 'paša', 'pust',\n",
       "       'teza', 'koda', 'predsodek', 'instrument', 'smuč', 'pečica',\n",
       "       'nestrpnost', 'lok', 'bob', 'obrt', 'kopanje', 'rez', 'pok',\n",
       "       'jarek', 'peščica', 'metal', 'palček', 'ruta', 'solo', 'hod'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../ccGigaFida/results/data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "with open('../ccGigaFida/results/data_lema.json') as json_file:\n",
    "    data_lema = json.load(json_file)\n",
    "with open('../ccGigaFida/results/data_pos.json') as json_file:\n",
    "    data_len = json.load(json_file)\n",
    "\n",
    "words = np.load(\"../ccGigaFida/words.npy\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_of_words=5\n",
    "for keyword in ['gol']:\n",
    "    all_sentences2=data[keyword][:100]\n",
    "    all_sentences_lema2=data_lema[keyword][:100]\n",
    "\n",
    "    all_sentences = []\n",
    "    all_sentences_lema = []\n",
    "    for sentence, sentence_lema in zip(all_sentences2, all_sentences_lema2):\n",
    "        if len(sentence) >= min_number_of_words and sentence_lema not in all_sentences_lema:\n",
    "            all_sentences.append(sentence)\n",
    "            all_sentences_lema.append(sentence_lema)\n",
    "            \n",
    "    all_embeddings=np.zeros((len(all_sentences),100))\n",
    "    \n",
    "    for i in range(len(all_sentences)): #iterate through the sentences for the given keyword\n",
    "        keyword_position = all_sentences_lema[i].index(keyword)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
