{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbLH5hRyyhh"
      },
      "source": [
        "# sloBERTa - TripleM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArxgCUZTy6JO"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9TGfkPj6dVbe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import gensim\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import SpectralCoclustering\n",
        "from scipy.stats import gaussian_kde\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN\n",
        "import torch\n",
        "from transformers import CamembertTokenizer, CamembertModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kPwKHrEzGAV"
      },
      "source": [
        "## Load sloBERTa 2.0 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riGHt5hTnHFZ",
        "outputId": "d9beece2-80ba-4673-fc66-bbf7768103c6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = CamembertTokenizer.from_pretrained(\"src/pretrained models/BERT/sloBERTa2/sloberta.2.0.transformers\")\n",
        "model = CamembertModel.from_pretrained(\"src/pretrained models/BERT/sloBERTa2/sloberta.2.0.transformers\", output_hidden_states = True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJKg2SzzIge"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l179Exjeq8Z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "with open('src/corpus/data.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "with open('src/corpus/data_lema.json') as json_file:\n",
        "    data_lema = json.load(json_file)\n",
        "with open('src/corpus/data_pos.json') as json_file:\n",
        "    data_len = json.load(json_file)\n",
        "\n",
        "words = np.load(\"src/corpus/words.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qP3q3bAzK3W"
      },
      "source": [
        "## Prepare sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOjOhruYthR6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "min_number_of_words = 8\n",
        "min_neighbor_distance = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-ykw00Jeyf_",
        "outputId": "ecfdf46d-5893-43ce-96b5-7c95761eb983",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "number_of_sentences_to_use = 3000\n",
        "our_word = \"golf\"\n",
        "\n",
        "for keyword in [our_word]:\n",
        "    all_sentences2=data[keyword][:min(number_of_sentences_to_use, len(data[keyword]))]\n",
        "    all_sentences_lema2=data_lema[keyword][:min(number_of_sentences_to_use, len(data[keyword]))]\n",
        "\n",
        "    all_sentences = []\n",
        "    all_sentences_lema = []\n",
        "    for sentence, sentence_lema in zip(all_sentences2, all_sentences_lema2):\n",
        "      if len(sentence) >= min_number_of_words:\n",
        "        if sentence_lema not in all_sentences_lema:\n",
        "          all_sentences.append(sentence)\n",
        "          all_sentences_lema.append(sentence_lema)\n",
        "\n",
        "\n",
        "\n",
        "    print(len(all_sentences))\n",
        "    \n",
        "    all_embeddings=np.zeros((len(all_sentences),768))\n",
        "    #all_embeddings = []\n",
        "    \n",
        "    for i in range(len(all_sentences)): #iterate through the sentences for the given keyword\n",
        "        sentence = ' '.join(all_sentences_lema[i])\n",
        "        keyword_position = all_sentences_lema[i].index(keyword)\n",
        "\n",
        "        marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "\n",
        "        tokenized_text = tokenizer.tokenize(marked_text)\n",
        "        segments_ids = [1] * len(tokenized_text)\n",
        "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        segments_tensors = torch.tensor([segments_ids])\n",
        "        with torch.no_grad():\n",
        "          outputs = model(tokens_tensor, segments_tensors)\n",
        "          hidden_states = outputs[2]\n",
        "          token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "          token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "          token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "          token_vecs_sum = []\n",
        "\n",
        "          for token in token_embeddings:\n",
        "              sum_vec = torch.sum(token[-4:], dim=0)\n",
        "              \n",
        "              token_vecs_sum.append(sum_vec)\n",
        "\n",
        "          all_embeddings[i,:]=token_vecs_sum[keyword_position].numpy()\n",
        "          \n",
        "    \n",
        "    break #this break means that we terminate on the first word\n",
        "    \n",
        "#~np.all(all_embeddings == 0, axis=1) this checks if there are any only-zero rows meaning that we did not have any word embedding for that sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPbnMBThhCpX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#Some utility functions which are used\n",
        "def cos_similarity(x, y):\n",
        "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "\n",
        "def remove_diplicate_sentences(a):\n",
        "    b_set = set(map(tuple,a))  #need to convert the inner lists to tuples so they are hashable\n",
        "    b = list(map(list,b_set)) #Now convert tuples back into lists (maybe unnecessary?)\n",
        "    return b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k3K0gd-hAWp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "distances={}\n",
        "for i in range(all_embeddings.shape[0]):\n",
        "    for j in range(i+1,all_embeddings.shape[0],1):\n",
        "        distances[str(i)+'-'+str(j)] = cos_similarity(all_embeddings[i,:], all_embeddings[j,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86h3OmmEXcck",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "distances = dict(sorted(distances.items(), key=lambda x:x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQb_FLJlj2Q"
      },
      "source": [
        "## Analyze worst 50 distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7yIoKjGllIQ",
        "outputId": "4bbeffe8-d748-4677-f328-29f8b1d58344",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "distances_keys = list(distances.keys())\n",
        "already_were = [] #to je zato ker se stavke ponavljajo in ne zanimajo nas iste kombinacije znova\n",
        "count=0\n",
        "for key in distances_keys[0:20]:\n",
        "    print(distances[key])\n",
        "    first, second = map(int, key.split('-'))\n",
        "    first_sent, second_sent = ' '.join(all_sentences[first]), ' '.join(all_sentences[second])\n",
        "    \n",
        "    if first_sent+second_sent in already_were:\n",
        "        continue\n",
        "    \n",
        "    count+=1\n",
        "    already_were.append(first_sent+second_sent)\n",
        "    print(first_sent+'\\n'+second_sent)\n",
        "    print('-----')\n",
        "    if count==20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWEhYKYpG8H5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "distances = dict(sorted(distances.items(), key=lambda x:x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBH7gJrnGrGz",
        "outputId": "760dadb9-512a-45da-c06a-700cdaee39ed",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "distances_keys = list(distances.keys())\n",
        "already_were = [] #to je zato ker se stavke ponavljajo in ne zanimajo nas iste kombinacije znova\n",
        "count=0\n",
        "for key in distances_keys[0:20]:\n",
        "    print(distances[key])\n",
        "    first, second = map(int, key.split('-'))\n",
        "    first_sent, second_sent = ' '.join(all_sentences[first]), ' '.join(all_sentences[second])\n",
        "    \n",
        "    if first_sent+second_sent in already_were:\n",
        "        continue\n",
        "    \n",
        "    count+=1\n",
        "    already_were.append(first_sent+second_sent)\n",
        "    print(first_sent+'\\n'+second_sent)\n",
        "    print('-----')\n",
        "    if count==20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tOvXp5wltjv"
      },
      "source": [
        "## Analyze best 50 distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3oBz8TclvH3",
        "outputId": "d15a5311-1d81-413a-cc34-081a76a66eb1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "distances_keys = list(distances.keys())\n",
        "already_were = [] #to je zato ker se stavke ponavljajo in ne zanimajo nas iste kombinacije znova\n",
        "count=0\n",
        "for key in distances_keys[-20:]:\n",
        "    print(distances[key])\n",
        "    first, second = map(int, key.split('-'))\n",
        "    first_sent, second_sent = ' '.join(all_sentences[first]), ' '.join(all_sentences[second])\n",
        "    if first_sent+second_sent in already_were:\n",
        "        continue\n",
        "    \n",
        "    count+=1\n",
        "    already_were.append(first_sent+second_sent)\n",
        "    print(first_sent+'\\n'+second_sent)\n",
        "    print('-----')\n",
        "    if count==20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXnJzRh0lzMZ"
      },
      "source": [
        "## Construct similarity distance matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRJ7usWXl0TW",
        "outputId": "2aef4d98-a220-471b-9449-f75b81e8834b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#print(all_embeddings.shape)\n",
        "distance_matrix=(pairwise_distances(all_embeddings,metric=\"cosine\"))\n",
        "similarity_matrix=1-distance_matrix\n",
        "\n",
        "\n",
        "#density plot\n",
        "similarity_matrix_flatten = similarity_matrix.reshape(len(similarity_matrix)**2)\n",
        "density = gaussian_kde(similarity_matrix_flatten)\n",
        "density.covariance_factor = lambda : .5\n",
        "len(similarity_matrix_flatten)\n",
        "#plt.plot(similarity_matrix_flatten, density(similarity_matrix_flatten))\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xolygXcLaCZz",
        "outputId": "216ecee7-f418-4c01-94be-09fbddff010f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "np.max(distance_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqqx5BAnaLMc",
        "outputId": "3d19e5cd-db6d-4fb6-e054-0ba2affc26b7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "np.min(similarity_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEwY7kRdmdZ1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "best_distances_indices=[]\n",
        "\n",
        "best_scores_indices=[]\n",
        "for i in list(distances.keys()):\n",
        "    first, second = i.split('-')\n",
        "    \n",
        "    best_scores_indices.append(first)\n",
        "    best_scores_indices.append(second)\n",
        "\n",
        "\n",
        "best_scores_indices=(np.unique(best_scores_indices))\n",
        "best_scores_indices = best_scores_indices.astype(int)\n",
        "all_embeddings_selected = all_embeddings[best_scores_indices, :]\n",
        "\n",
        "distance_matrix=(pairwise_distances(all_embeddings_selected,metric=\"cosine\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvqNkvhUmfRz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "all_sentences_selected = []\n",
        "for i in best_scores_indices:\n",
        "    all_sentences_selected.append(all_sentences[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH2N2k3Mmo_b"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2puzTZwmmYV",
        "outputId": "e5096ac6-1645-4b64-b8ba-42845c5784a9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "clusters = AgglomerativeClustering(affinity='precomputed', linkage='complete', n_clusters=2).fit(distance_matrix)\n",
        "print(Counter(clusters.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqkCZj_PyRhW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "dataset = np.hstack((all_embeddings_selected, np.transpose([clusters.labels_])))\n",
        "columns = list(range(dataset.shape[1]))\n",
        "columns[-1] = \"y\"\n",
        "df = pd.DataFrame(dataset, columns = columns)\n",
        "df.to_csv(f\"/mydrive/results/{our_word}_hierarhical_sloBERTa.csv\", index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfWh1Q1jtrdH",
        "outputId": "b63a2bc2-6c92-42d0-8a3f-a10f12614d94",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "for i,sent in enumerate(all_sentences):\n",
        "  print(i,\":\", ' '.join(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT3U-ZIamvx8",
        "outputId": "ced8bcab-3d90-431a-fe77-a45b3da81036",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "for i,label in enumerate(clusters.labels_):\n",
        "    if label==1:\n",
        "        print(i,':', ' '.join(all_sentences_selected[i]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "sloBERTa 2.0 - tripleM .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
