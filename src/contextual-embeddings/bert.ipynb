{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast text approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "import zipfile\n",
    "import gensim\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load slovene BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/matej/Documents/FRI/NLP/NLP-tripleM/src/pretrained models/BERT/sloBERTa2/sloberta.2.0.transformers were not used when initializing CamembertModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertModel were not initialized from the model checkpoint at /home/matej/Documents/FRI/NLP/NLP-tripleM/src/pretrained models/BERT/sloBERTa2/sloberta.2.0.transformers and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "#CamembertTokenizer.from_pretrained\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"/home/matej/Documents/FRI/NLP/NLP-tripleM/src/pretrained models/BERT/sloBERTa2/sloberta.2.0.transformers\")\n",
    "model = CamembertModel.from_pretrained(\"/home/matej/Documents/FRI/NLP/NLP-tripleM/src/pretrained models/BERT/sloBERTa2/sloberta.2.0.transformers\", output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁[', 'C', 'LS', ']', '▁Af', 'ter', '▁ste', 'ali', 'ng', '▁mo', 'ney', '▁from', '▁the', '▁bank', '▁va', 'ult', ',', '▁the', '▁bank', '▁rob', 'ber', '▁was', '▁se', 'en', '▁fi', 's', 'hing', '▁on', '▁the', '▁Mi', 'ssi', 'ssi', 'p', 'pi', '▁ri', 'ver', '▁bank', '.', '▁[', 'SE', 'P', ']']\n",
      "▁[              668\n",
      "C            31,833\n",
      "LS            7,358\n",
      "]            31,858\n",
      "▁Af           9,706\n",
      "ter             219\n",
      "▁ste            794\n",
      "ali             784\n",
      "ng              823\n",
      "▁mo             104\n",
      "ney          16,362\n",
      "▁from        21,330\n",
      "▁the          2,581\n",
      "▁bank         5,198\n",
      "▁va             316\n",
      "ult          10,772\n",
      ",            31,791\n",
      "▁the          2,581\n",
      "▁bank         5,198\n",
      "▁rob         10,936\n",
      "ber           1,083\n",
      "▁was         31,731\n",
      "▁se              49\n",
      "en              410\n",
      "▁fi             341\n",
      "s            31,781\n",
      "hing          7,768\n",
      "▁on           2,708\n",
      "▁the          2,581\n",
      "▁Mi             537\n",
      "ssi           4,046\n",
      "ssi           4,046\n",
      "p            31,787\n",
      "pi              128\n",
      "▁ri           1,216\n",
      "ver             981\n",
      "▁bank         5,198\n",
      ".            31,795\n",
      "▁[              668\n",
      "SE            3,607\n",
      "P            31,803\n",
      "]            31,858\n"
     ]
    }
   ],
   "source": [
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "index_of_vault = text.index(\"vault\")\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "print(tokenized_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print (segments_ids)\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 42\n",
      "Number of hidden units: 768\n",
      "Number of hidden units: tensor(0.2199)\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
    "print (\"Number of hidden units:\", hidden_states[layer_i][batch_i][token_i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "token_vault = hidden_states[-2][0][index_of_vault]\n",
    "print (\"Our final sentence embedding vector of shape:\", token_vault.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3619e+00, -5.4224e-01,  3.9113e-04, -1.5025e-01, -3.5773e-01,\n",
       "         3.9550e-01, -4.0271e-01, -1.4918e-01, -1.7332e-01,  8.7076e-02,\n",
       "        -4.5890e-01, -5.0499e-01,  3.9831e-01, -9.6095e-02,  7.4271e-03,\n",
       "        -3.5406e-01,  3.0654e-01,  2.0857e-01,  4.8039e-01, -1.1335e-01,\n",
       "         6.4723e-01,  1.5501e-01,  1.9120e-01,  3.6621e-01, -2.8977e-01,\n",
       "         2.6011e-01,  1.9661e-01, -8.2372e-01,  1.6198e-01,  3.5347e-02,\n",
       "        -1.8555e-01,  4.1205e-01, -3.5163e-01, -1.5201e-01, -4.2921e-01,\n",
       "        -2.6648e-01,  1.1602e-01,  1.1035e-01, -3.3887e-01,  3.6624e-01,\n",
       "        -2.5834e-01, -2.3587e-01,  8.2413e-01,  1.5979e-01, -1.3532e-02,\n",
       "         1.2285e+00, -1.1470e-01,  5.4101e-01,  3.1784e-01, -8.6554e-01,\n",
       "         1.2012e-01,  2.3612e-01,  4.7665e-02,  1.5308e-01,  9.1470e-02,\n",
       "        -4.3538e-01,  4.3885e-01, -6.6993e-01, -6.4054e-01, -5.5695e-02,\n",
       "        -3.9838e-01,  9.1519e-02,  6.8036e-01,  2.7102e-01, -2.2802e-01,\n",
       "         3.9754e-01,  1.1384e+00,  1.5691e-01,  1.4933e-02, -9.9915e-01,\n",
       "        -5.3042e-01, -2.7664e-01,  1.1767e-01, -1.9704e-01, -2.7350e-01,\n",
       "        -2.4190e-01,  1.6551e-01,  3.2017e-01, -3.1101e-01,  4.5571e-01,\n",
       "        -3.6825e-01,  2.4370e-01,  3.6854e-01,  7.6982e-02, -3.2066e-01,\n",
       "         9.3773e-02, -2.8397e-01,  1.1251e-01, -1.5705e-01,  1.7149e-01,\n",
       "         2.1770e-01, -1.2880e-02,  2.2218e-01,  5.1136e-01, -2.2672e-01,\n",
       "         5.0596e-02, -8.3253e-02,  2.2832e-01,  2.1372e-01,  8.7154e-02,\n",
       "        -4.1723e-01,  9.6290e-02, -3.2806e-01, -8.2655e-02, -5.0367e-01,\n",
       "        -1.2494e-01,  6.0739e-02, -8.8578e-01,  5.1272e-01,  3.2526e-01,\n",
       "        -3.9453e-01, -1.2309e-01,  1.2654e-01, -1.7843e-01,  5.7693e-01,\n",
       "        -7.9211e-03, -4.3631e-01, -2.1043e-01,  3.3997e-01, -4.4715e-01,\n",
       "        -7.8619e-01,  3.2971e-01,  5.5780e-02, -7.4123e-01,  1.4477e-01,\n",
       "        -5.3504e-01,  5.2201e-01,  1.2046e-01,  3.2172e-01, -6.8126e-01,\n",
       "         1.7651e-01,  4.6404e-01, -2.2515e-01,  6.3202e-03, -8.8294e-02,\n",
       "         1.3144e-01,  5.9906e-01, -6.4288e-01, -2.8479e-01, -3.7352e-01,\n",
       "         3.7960e-01, -4.9293e-02,  5.2402e-02, -5.4848e-01,  1.3595e-02,\n",
       "         2.1930e-01,  5.3795e-01, -2.3542e-01, -1.0472e-01, -2.7302e-01,\n",
       "         7.4285e-01, -6.7562e-01, -2.9565e-02, -5.5609e-01, -5.8236e-01,\n",
       "        -8.3226e-01, -3.0661e-01, -3.3359e-01,  1.1425e-01,  2.7502e-01,\n",
       "         1.3989e-01,  5.3444e-01,  2.3979e-01,  1.3421e-01,  9.5120e-03,\n",
       "        -8.6246e-02,  1.1787e-01,  3.7828e-01, -1.9916e-01, -2.8601e-01,\n",
       "        -9.3915e-03,  4.5860e-01, -3.0417e-01, -3.3389e-02,  3.3037e-01,\n",
       "         2.8532e-01, -8.9100e-02, -2.1077e+00, -1.6067e-01, -7.1198e-01,\n",
       "         4.8299e-01,  1.2232e-01, -2.3466e-01, -3.7880e-01,  1.5377e-01,\n",
       "        -2.7909e-01, -4.2843e-03,  2.5251e-01, -6.3949e-01, -3.5713e-01,\n",
       "        -3.1197e-01, -4.5023e-01,  4.9257e-02,  8.7671e-01, -3.1338e-01,\n",
       "         2.7273e-01, -1.2431e-01,  2.1936e-01, -6.9770e-02, -1.1190e+00,\n",
       "         7.0394e-01,  5.1368e-01, -4.4385e-02,  3.4703e-02, -2.1841e-01,\n",
       "         3.8086e-02,  1.5181e-01, -1.2751e-01,  5.4223e-01, -4.1346e-01,\n",
       "        -1.2532e-01, -7.2445e-02, -1.7311e-01,  4.0180e-01,  4.2720e-02,\n",
       "         4.6324e-01, -3.7594e-01, -1.9172e-01,  2.4396e-02,  3.5554e-01,\n",
       "        -1.4257e-01, -2.9711e-01,  4.7408e-01, -5.5707e-01, -2.1377e-01,\n",
       "        -6.2782e-01, -4.0278e-01, -3.4632e-01,  5.1405e-01, -2.8625e-01,\n",
       "         7.0046e-02,  1.5893e+00, -9.4701e-01,  5.6307e-01, -3.9460e-02,\n",
       "        -2.7785e-01, -1.9179e-01,  1.7729e-01, -2.6803e-01,  1.2577e-01,\n",
       "         2.4617e-01, -1.6419e-02, -1.4870e-01, -2.3908e-01, -6.4813e-01,\n",
       "        -1.1255e-01, -1.3222e-01, -4.0049e-02,  4.3948e-02,  2.5868e-01,\n",
       "        -2.0816e-01, -3.8369e-01, -2.4309e-01, -3.6963e-01,  3.0156e-01,\n",
       "        -1.2630e-01,  8.0918e-01, -1.2968e-01, -5.5961e-01,  6.0850e-02,\n",
       "         4.0599e-01,  6.1671e-01,  9.1640e-01,  1.2904e-01, -1.7052e-01,\n",
       "         2.0709e-01, -3.3691e-02,  8.1162e-02,  3.7948e-01, -1.7259e-01,\n",
       "         4.0457e-01, -2.3830e-01,  5.9991e-01, -1.1101e-01, -1.7686e-01,\n",
       "        -1.9663e-01,  8.3342e-02, -7.0366e-02,  1.5935e-02,  5.9830e-02,\n",
       "         5.4170e-01,  9.3583e-01, -6.4805e-01, -8.6471e-02,  1.7169e-01,\n",
       "         1.6099e-01, -3.9235e-01,  6.3769e-02, -1.7189e-01, -6.3083e-01,\n",
       "        -3.7310e-01,  3.6119e-02,  5.5317e-01, -2.4813e-01, -7.2439e-01,\n",
       "         1.5908e-01,  4.1776e-01,  6.4965e-02, -1.6765e-01, -6.8741e-04,\n",
       "        -3.4401e-01,  5.6147e-02,  4.1927e-01,  1.4922e+00,  6.6801e-01,\n",
       "         5.1936e-02, -4.9222e-02,  8.5397e-02, -7.7510e-01,  7.4709e-01,\n",
       "         6.1030e-01, -5.6334e-01, -3.6968e-01,  1.8018e-02, -3.6256e-01,\n",
       "         4.7662e-01, -7.7421e-01, -4.1695e-01,  1.6924e-01, -1.0269e+00,\n",
       "         1.0860e-01,  3.6870e-01, -2.1766e-01, -4.2414e-01,  4.4921e-01,\n",
       "        -1.7933e-01,  3.6045e-01, -2.8455e-01,  2.4270e-02,  7.3429e-01,\n",
       "        -2.0695e-01, -3.0811e-01,  4.7371e-01, -6.2234e-02, -3.8697e-01,\n",
       "        -3.1917e-01,  4.0498e-01, -3.5729e-01, -6.0420e-01, -5.9785e-01,\n",
       "        -1.8168e-01, -4.3668e-01, -8.6020e-01, -6.4072e-02, -7.7225e-02,\n",
       "         6.4215e-02, -2.9007e-01,  1.0162e-01,  3.5273e-01,  4.1650e-01,\n",
       "         6.1290e-01,  3.4872e-01, -1.9998e-01,  2.9124e-02,  1.6559e-01,\n",
       "        -2.9077e-01,  4.0148e-01, -6.2973e-02, -2.2048e-01, -1.5654e-01,\n",
       "        -1.0323e-01,  1.9226e-01,  5.9955e-01, -5.6026e-01,  2.9831e-01,\n",
       "        -7.2447e-01, -3.7294e-01, -1.7586e-01,  2.1782e-01, -4.2696e-01,\n",
       "         1.1492e+00, -3.8475e-01,  2.0990e-01,  2.0286e-01, -2.6596e-01,\n",
       "         4.6975e-01,  6.3099e-01,  9.4155e-02, -8.1604e-02, -2.5036e-01,\n",
       "        -5.7041e-01,  4.2957e-01, -4.4785e-01,  2.0892e-01,  4.1675e-01,\n",
       "        -2.3722e-01,  2.4957e-01,  1.4806e-01, -4.9976e-02, -3.2931e-01,\n",
       "        -1.1029e-01, -8.2022e-02, -2.8238e-01, -2.5238e-01, -1.4030e+00,\n",
       "         1.0967e-01,  5.6398e-01, -2.2109e-01, -2.7022e-01, -1.4948e-01,\n",
       "        -1.9761e-01,  2.5969e-01,  2.2918e-01,  3.1039e-01,  1.6975e-01,\n",
       "         1.3405e-01,  9.2611e-01,  6.7908e-02, -1.6985e-01,  3.5550e-01,\n",
       "         1.7662e-02, -7.9702e-02,  4.3768e-01, -1.3620e-01, -3.2324e-01,\n",
       "         7.8002e-01,  9.2089e-02, -2.5632e-01, -2.1915e-02, -3.5180e-01,\n",
       "        -9.0042e-03, -1.9800e-01,  4.6507e-01,  3.5171e-01, -3.6538e-01,\n",
       "         3.5326e-01,  6.5148e-02, -3.4488e-01, -4.5657e-01, -3.1940e-01,\n",
       "         2.4201e-01,  3.2199e-01,  1.2495e+00,  1.5985e-01, -1.4597e-01,\n",
       "        -2.1747e-01,  1.0037e-01,  7.3056e-01, -1.0525e-01, -6.5795e-02,\n",
       "        -7.1565e-02,  2.5491e-01, -7.0061e-01, -2.7936e-01,  3.6314e-01,\n",
       "         6.5804e-01,  3.6446e-01,  1.7890e-01, -1.1991e-01,  8.6727e-02,\n",
       "         1.0746e-01, -3.0320e-01, -1.5209e-02, -1.4983e-01, -1.2019e-01,\n",
       "        -9.6813e-01,  2.6850e-01, -1.7216e-01, -9.7695e-02,  3.9810e-01,\n",
       "        -7.6715e-02,  4.4818e-01,  8.5683e-01, -6.2391e-01,  3.4472e-01,\n",
       "         3.4411e-02, -2.5536e-01, -4.0106e-01,  5.4070e-01,  2.0527e+00,\n",
       "         6.1391e-01,  1.5689e-02,  1.3770e-01, -2.0677e-01, -6.9082e-01,\n",
       "         9.3059e-01, -2.9547e-02,  7.6195e-01, -3.1409e-01, -9.5059e-01,\n",
       "        -3.2692e-01, -6.7732e-01,  2.7986e-01,  2.8157e-01, -8.3658e-02,\n",
       "         8.4758e-01, -2.1638e-01,  9.5719e-02, -1.1645e-01, -2.2812e-01,\n",
       "         2.4736e-01,  1.9222e-01, -3.3916e-01,  5.6943e-01,  2.6587e-01,\n",
       "        -8.8329e-01, -2.2296e-01, -1.8237e-01, -3.5111e-01, -9.0588e-02,\n",
       "         4.1156e-01,  2.7509e-01, -3.4408e-01,  4.2805e-01, -3.5231e-01,\n",
       "        -1.6791e-02, -1.3050e-01,  4.1113e-01, -7.8352e-02, -2.8545e-02,\n",
       "         3.8306e-01,  3.3665e-01,  5.7163e-01,  3.4765e-01, -5.3221e-01,\n",
       "        -3.3988e-01,  5.7883e-01,  8.7604e-01, -7.6947e-01,  3.9219e-01,\n",
       "        -7.5043e-02, -2.2076e-01, -1.2044e-01, -6.3616e-01, -2.8529e-01,\n",
       "         1.3864e-01, -5.4151e-02, -1.0914e-01, -5.0388e-01, -4.2510e-01,\n",
       "        -3.3708e-01,  1.6113e-01, -2.3945e-01,  1.1337e+00,  2.9831e-01,\n",
       "         2.4296e-01,  4.9172e-02, -4.7598e-01,  7.0405e-02,  6.2721e-02,\n",
       "        -1.9934e-01,  7.7878e-01,  1.3784e-01,  4.8246e-02,  4.5769e-01,\n",
       "         1.3413e-01, -6.8082e-03, -4.7372e-02, -8.3569e-02, -2.6592e-01,\n",
       "         3.6850e-01,  3.3901e-01,  8.2768e-01,  2.5182e-01,  6.2649e+00,\n",
       "         1.4059e-01, -1.2740e-01, -1.9971e-01,  2.3819e-01,  1.5605e-01,\n",
       "        -6.0305e-01, -2.6416e-01,  4.4217e-01,  2.1390e-01,  5.7565e-01,\n",
       "         9.1990e-01,  1.8694e-02, -2.5705e-01, -1.7278e-01, -6.7974e-02,\n",
       "         2.1813e-01, -4.4220e-01,  4.6333e-01, -1.0056e-01, -4.0548e-01,\n",
       "        -1.7357e-01,  9.6761e-01,  5.2732e-02, -2.2012e-01, -2.6932e-01,\n",
       "        -6.7740e-02,  3.1399e-01, -5.0137e-01, -3.8562e-03, -2.8259e-03,\n",
       "        -4.5536e-02, -8.1705e-01, -1.5715e-01, -2.9352e-01, -3.4572e-01,\n",
       "        -2.5611e-01, -4.2363e-01, -3.8005e-03, -1.8700e-01,  3.2510e-01,\n",
       "         8.0332e-02, -3.5218e-01,  7.1577e-02, -1.0509e-01,  7.7810e-01,\n",
       "         1.2673e+00, -1.5972e-01,  3.8043e-01,  5.8528e-01,  6.2134e-01,\n",
       "         1.9866e-01,  4.3756e-03, -6.1394e-02,  1.1755e-01, -3.3560e-01,\n",
       "        -3.0156e-01, -1.3591e-02, -9.0265e-02, -4.2751e-01, -2.7578e-02,\n",
       "         2.8299e-01,  1.1572e-01, -4.4775e-01, -5.6970e-01, -3.5171e-02,\n",
       "         1.3805e-02, -4.2958e-01, -3.5341e-01,  4.4359e-01,  3.1685e-02,\n",
       "         6.6157e-02,  2.2251e-01, -1.0529e+00, -3.0468e-01, -3.7030e-01,\n",
       "        -3.0795e-02, -2.1241e-01, -5.9667e-02,  2.4682e-01, -3.8150e-01,\n",
       "        -3.7966e-01, -4.6583e-01, -3.2784e-01,  5.6591e-01, -1.9001e+00,\n",
       "         8.5567e-02,  5.4565e-01,  3.0230e-01,  1.8749e-01, -5.1191e-01,\n",
       "         6.9966e-02, -2.1925e-01, -4.1212e-02, -8.7835e-01, -6.6799e-01,\n",
       "        -2.0683e-01,  4.3447e-01,  1.4163e-01, -4.6319e-01, -4.1210e-01,\n",
       "         1.1493e-01, -6.4544e-01,  2.7259e-01,  8.5655e-02, -4.5890e-01,\n",
       "        -2.6160e-01, -2.4955e-01,  3.4934e-01,  2.0872e-01, -2.1582e-01,\n",
       "         1.4363e-01,  1.3671e-02,  4.6763e-01,  1.1748e-01,  3.5929e-02,\n",
       "        -2.4580e-01, -6.6168e-04, -1.8678e-02,  5.2436e-01, -5.6321e-01,\n",
       "         6.5273e-03, -5.1294e-01,  1.4641e-02,  2.3031e-01, -6.4942e-02,\n",
       "         5.4092e-03,  4.2859e-01, -1.2262e-01,  1.7425e-02,  2.2254e-01,\n",
       "         1.0708e-01,  9.7939e-02,  1.6913e-01,  9.6997e-01, -1.2993e-01,\n",
       "        -6.1305e-01,  3.6806e-02, -3.7542e-01, -1.8493e-01,  4.9555e-02,\n",
       "         4.1329e-01,  1.9668e-01, -4.5668e-02, -6.9877e-04,  4.2189e-01,\n",
       "        -1.8693e-01, -2.4297e-01,  8.5184e-02, -1.4404e-01, -6.3659e-01,\n",
       "        -5.0767e-01, -2.1833e-01, -4.8093e-01, -2.5202e-01,  3.4165e-01,\n",
       "         3.7900e-01,  3.6692e-01,  3.3498e-01, -6.3078e-01, -1.7013e+00,\n",
       "        -6.0019e-01, -4.0656e-02,  2.6040e-01, -7.9584e-02,  3.0963e-01,\n",
       "         7.5326e-01, -2.5879e-01,  7.2097e-01, -2.2985e-01, -9.4420e-01,\n",
       "        -6.1874e-01,  2.3311e-01,  1.8705e-01, -2.0841e-01,  3.6948e-01,\n",
       "        -1.6666e-01, -1.6959e-01, -2.7917e-02,  1.5400e-01,  6.0762e-01,\n",
       "         1.1067e-02, -1.1661e-01, -1.1665e-02,  2.7945e-01, -1.0157e+00,\n",
       "        -4.4848e-02, -1.3598e-01,  1.4251e-01, -8.0608e-02,  9.6973e-01,\n",
       "         1.1074e-02, -2.6763e-01, -1.7272e+00, -5.8689e-02,  1.0347e-01,\n",
       "         2.1205e-01,  1.6120e-02, -1.4017e-01, -1.0363e-01, -6.0791e-02,\n",
       "         3.9497e-01, -2.9987e-01,  1.7144e+00, -4.6795e-01,  1.7458e-01,\n",
       "         6.4402e-01,  2.6346e-01, -5.0176e-02,  2.8272e-01,  7.3100e-01,\n",
       "         5.1468e-02, -3.4984e-01,  4.2058e-01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vault"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "004cf412bede8e677bcf59b14e1e48ce17b57f17e644433ba0ac6b9fba05c9bd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp-trimpleM': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
